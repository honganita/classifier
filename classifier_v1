import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import pytz
from scipy import stats

# Load and prepare SOFR data
def load_sofr_data(bquery, start_date, end_date):
    df = bquery.bdib('USOSFR10 BGN Curncy', event_type='TRADE', interval=60, 
                    start_datetime=start_date, end_datetime=end_date)
    df['time'] = pd.to_datetime(df['time'])
    df['diff'] = df['high'] - df['low']
    df['returns'] = df['close'].pct_change()  # Add returns for additional volatility measure
    return df[['time', 'high', 'low', 'close', 'diff', 'returns']]  # Include all needed columns

# Load and prepare economic events
def load_economic_events(filepath):
    data = pd.read_csv(filepath)
    data = data.rename(columns={
        'Date Time': 'time',
        'Country Code': 'country',
        'Event': 'event',
        'Ticker': 'ticker'
    })
    data['time'] = pd.to_datetime(data['time'], errors='coerce')
    data = data[['time', 'country', 'event', 'ticker']].dropna(subset=['time'])
    return data[data['country'] == 'US']

# Calculate windowed volatility metrics
def calculate_window_metrics(sofr_df, event_time, window_hours=5):
    """
    Calculate volatility metrics for a window around an event time
    window_hours: total hours to consider (split evenly before/after event)
    """
    half_window = window_hours / 2
    start_window = event_time - timedelta(hours=half_window)
    end_window = event_time + timedelta(hours=half_window)
    
    window_data = sofr_df[
        (sofr_df['time'] >= start_window) & 
        (sofr_df['time'] <= end_window)
    ].copy()
    
    if len(window_data) == 0:
        return None
    
    # Calculate metrics
    window_data['hour_offset'] = (window_data['time'] - event_time).dt.total_seconds() / 3600
    
    metrics = {
        'max_diff': window_data['diff'].max(),
        'total_range': window_data['high'].max() - window_data['low'].min(),
        'avg_diff': window_data['diff'].mean(),
        'volatility': window_data['returns'].std(),
        'max_abs_return': window_data['returns'].abs().max(),
        'event_impact': window_data['close'].iloc[-1] - window_data['close'].iloc[0]
    }
    
    return {**metrics, 'window_data': window_data}

# Calculate baseline volatility characteristics
def calculate_baseline_metrics(sofr_df, sample_size=1000, window_hours=5):
    """Calculate baseline distribution of volatility metrics"""
    baseline_metrics = []
    available_times = sofr_df['time'].unique()
    
    for _ in range(sample_size):
        random_time = pd.to_datetime(np.random.choice(available_times))  # Convert to Timestamp
        metrics = calculate_window_metrics(sofr_df, random_time, window_hours)
        if metrics:
            baseline_metrics.append({k: v for k, v in metrics.items() if k != 'window_data'})
    
    return pd.DataFrame(baseline_metrics)

# Score events against baseline
def score_events(event_metrics_df, baseline_df):
    """Score events based on how they compare to baseline volatility"""
    scores = []
    
    for col in event_metrics_df.columns:
        if col in baseline_df.columns:
            # Calculate percentile vs baseline
            scores.append(event_metrics_df[col].rank(pct=True).to_frame(col + '_score'))
    
    scores_df = pd.concat(scores, axis=1)
    
    # Composite score with weights
    weights = {
        'max_diff_score': 0.25,
        'total_range_score': 0.25,
        'volatility_score': 0.3,
        'max_abs_return_score': 0.2
    }
    
    scores_df['composite_score'] = sum(
        scores_df[col] * weight for col, weight in weights.items()
    )
    
    return scores_df

# Main analysis function
def analyze_events(sofr_df, events_df, window_hours=5):
    # Calculate baseline metrics
    print("Calculating baseline volatility distribution...")
    baseline_df = calculate_baseline_metrics(sofr_df, window_hours=window_hours)
    
    # Process each event
    print("Processing economic events...")
    all_event_metrics = []
    all_window_data = []
    
    for _, event in events_df.iterrows():
        metrics = calculate_window_metrics(sofr_df, event['time'], window_hours)
        if metrics:
            window_data = metrics.pop('window_data')
            window_data['event'] = event['event']
            window_data['ticker'] = event['ticker']
            all_window_data.append(window_data)
            
            event_metrics = {
                'time': event['time'],
                'event': event['event'],
                'ticker': event['ticker'],
                **metrics
            }
            all_event_metrics.append(event_metrics)
    
    event_metrics_df = pd.DataFrame(all_event_metrics)
    window_data_df = pd.concat(all_window_data)
    
    # Score events
    print("Scoring events...")
    scores_df = score_events(event_metrics_df.drop(columns=['time', 'event', 'ticker']), baseline_df)
    results_df = pd.concat([
        event_metrics_df[['time', 'event', 'ticker']],
        event_metrics_df.drop(columns=['time', 'event', 'ticker']),
        scores_df
    ], axis=1)
    
    # Calculate statistical significance
    for metric in ['max_diff', 'volatility', 'max_abs_return']:
        results_df[f'{metric}_pvalue'] = [
            stats.percentileofscore(baseline_df[metric], x)/100 
            for x in results_df[metric]
        ]
    
    return results_df, window_data_df, baseline_df

# Visualization functions
def plot_event_impacts(window_data_df):
    plot_df = window_data_df.groupby(['hour_offset', 'event'])['diff'].mean().reset_index()
    
    plt.figure(figsize=(14, 8))
    sns.lineplot(
        data=plot_df,
        x='hour_offset',
        y='diff',
        hue='event'
    )
    plt.title('SOFR 10Y Swap Rate Volatility Around Economic Events')
    plt.axvline(0, color='red', linestyle='--', label='Event Time')
    plt.xlabel('Hours Relative to Event')
    plt.ylabel('Price Range (High-Low)')
    plt.legend(bbox_to_anchor=(1.05, 1))
    plt.tight_layout()
    plt.show()

def plot_top_events(results_df, window_data_df, n=5):
    top_events = results_df.nlargest(n, 'composite_score')['event'].unique()
    plot_data = window_data_df[window_data_df['event'].isin(top_events)]
    
    # Remove any problematic parameters that might be passed through
    plot_kwargs = {
        'x': 'hour_offset',
        'y': 'diff',
        'errorbar': None,  # Disable error bars to avoid the issue
        'estimator': None  # Disable any aggregation
    }
    
    g = sns.FacetGrid(plot_data, col='event', col_wrap=3, height=4, sharey=False)
    g.map_dataframe(
        sns.lineplot,
        **plot_kwargs
    )
    g.set_titles('{col_name}')
    g.set_axis_labels('Hours Relative to Event', 'Price Range')
    for ax in g.axes.flat:
        ax.axvline(0, color='red', linestyle='--')
    plt.tight_layout()
    plt.show()


# Main execution
if __name__ == '__main__':
    # Initialize connection
    today = datetime.now(pytz.timezone('Asia/Hong_Kong'))
    bquery = blp.BlpQuery(
        host='usvxapificcl01.sdi.corp.bankofamerica.com',
        port=8194,
        uuid=32348716,
        ip='165.40.198.238'
    ).start()
    
    # Load data
    sofr_df = load_sofr_data(bquery, '2024-09-26', '2025-04-11')
    events_df = load_economic_events('half_year_data.csv')
    
    # Analyze events
    results_df, window_data_df, baseline_df = analyze_events(sofr_df, events_df)
    
    # Identify significant events (top 10%)
    threshold = results_df['composite_score'].quantile(0.9)
    major_events = results_df[results_df['composite_score'] >= threshold]
    
    # Save and display results
    major_events.to_csv('significant_economic_events.csv', index=False)
    print(f"Found {len(major_events)} significant economic events:")
    print(major_events[['time', 'event', 'composite_score']].sort_values('composite_score', ascending=False))
    
    # Visualizations
    plot_event_impacts(window_data_df)
    plot_top_events(results_df, window_data_df)
