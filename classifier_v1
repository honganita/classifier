import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import pytz
from scipy import stats

# Modified to load multiple instruments
def load_sofr_data(bquery, start_date, end_date):
    # Load both 5-year and 10-year SOFR swaps
    df5 = bquery.bdib('USOSFR5 BGN Curncy', event_type='TRADE', interval=60,
                     start_datetime=start_date, end_datetime=end_date)
    df10 = bquery.bdib('USOSFR10 BGN Curncy', event_type='TRADE', interval=60,
                      start_datetime=start_date, end_datetime=end_date)
    
    # Process 5-year data
    df5['time'] = pd.to_datetime(df5['time'])
    df5['diff'] = df5['high'] - df5['low']
    df5['returns'] = df5['close'].pct_change()
    df5['instrument'] = 'USOSFR5'
    
    # Process 10-year data
    df10['time'] = pd.to_datetime(df10['time'])
    df10['diff'] = df10['high'] - df10['low']
    df10['returns'] = df10['close'].pct_change()
    df10['instrument'] = 'USOSFR10'
    
    # Combine both instruments
    combined_df = pd.concat([df5[['time', 'high', 'low', 'close', 'diff', 'returns', 'instrument']],
                           df10[['time', 'high', 'low', 'close', 'diff', 'returns', 'instrument']]])
    
    return combined_df

# Modified window metrics calculation to handle multiple instruments
def calculate_window_metrics(sofr_df, event_time, window_hours=5):
    half_window = window_hours / 2
    start_window = event_time - timedelta(hours=half_window)
    end_window = event_time + timedelta(hours=half_window)
    
    window_data = sofr_df[
        (sofr_df['time'] >= start_window) & 
        (sofr_df['time'] <= end_window)
    ].copy()
    
    if len(window_data) == 0:
        return None
    
    window_data['hour_offset'] = (window_data['time'] - event_time).dt.total_seconds() / 3600
    
    # Calculate metrics for each instrument separately
    metrics = {}
    for instrument in window_data['instrument'].unique():
        inst_data = window_data[window_data['instrument'] == instrument]
        metrics[f'{instrument}_max_diff'] = inst_data['diff'].max()
        metrics[f'{instrument}_total_range'] = inst_data['high'].max() - inst_data['low'].min()
        metrics[f'{instrument}_avg_diff'] = inst_data['diff'].mean()
        metrics[f'{instrument}_volatility'] = inst_data['returns'].std()
        metrics[f'{instrument}_max_abs_return'] = inst_data['returns'].abs().max()
        metrics[f'{instrument}_event_impact'] = inst_data['close'].iloc[-1] - inst_data['close'].iloc[0]
    
    return {**metrics, 'window_data': window_data}

# Modified plotting functions
def plot_event_impacts(window_data_df):
    plt.figure(figsize=(16, 8))
    sns.lineplot(
        data=window_data_df,
        x='hour_offset',
        y='diff',
        hue='event',
        style='instrument',
        ci=None,
        estimator='median'
    )
    plt.title('SOFR Swap Rate Volatility Around Economic Events (5Y vs 10Y)')
    plt.axvline(0, color='red', linestyle='--', label='Event Time')
    plt.xlabel('Hours Relative to Event')
    plt.ylabel('Price Range (High-Low)')
    plt.legend(bbox_to_anchor=(1.05, 1))
    plt.tight_layout()
    plt.show()

def plot_top_events(results_df, window_data_df, n=5):
    top_events = results_df.nlargest(n, 'composite_score')['event'].unique()
    plot_data = window_data_df[window_data_df['event'].isin(top_events)]
    
    g = sns.FacetGrid(plot_data, col='event', row='instrument', height=4, aspect=1.5)
    g.map_dataframe(
        sns.lineplot,
        x='hour_offset',
        y='diff',
        ci=None
    )
    g.set_titles('{row_name} | {col_name}')
    g.set_axis_labels('Hours Relative to Event', 'Price Range')
    for ax in g.axes.flat:
        ax.axvline(0, color='red', linestyle='--')
    plt.tight_layout()
    plt.show()

# Modified analysis function
def analyze_events(sofr_df, events_df, window_hours=5):
    print("Calculating baseline volatility distribution...")
    baseline_df = calculate_baseline_metrics(sofr_df, window_hours=window_hours)
    
    print("Processing economic events...")
    all_event_metrics = []
    all_window_data = []
    
    for _, event in events_df.iterrows():
        metrics = calculate_window_metrics(sofr_df, event['time'], window_hours)
        if metrics:
            window_data = metrics.pop('window_data')
            window_data['event'] = event['event']
            window_data['ticker'] = event['ticker']
            all_window_data.append(window_data)
            
            event_metrics = {
                'time': event['time'],
                'event': event['event'],
                'ticker': event['ticker'],
                **metrics
            }
            all_event_metrics.append(event_metrics)
    
    event_metrics_df = pd.DataFrame(all_event_metrics)
    window_data_df = pd.concat(all_window_data)
    
    print("Scoring events...")
    # Create composite score that considers both instruments
    score_columns = [col for col in event_metrics_df.columns 
                    if any(x in col for x in ['max_diff', 'volatility', 'max_abs_return'])]
    
    scores = []
    for col in score_columns:
        scores.append(event_metrics_df[col].rank(pct=True).to_frame(col + '_score'))
    
    scores_df = pd.concat(scores, axis=1)
    
    # Weighted composite score (giving equal weight to both instruments)
    weights = {
        'USOSFR5_max_diff_score': 0.15,
        'USOSFR5_volatility_score': 0.2,
        'USOSFR5_max_abs_return_score': 0.15,
        'USOSFR10_max_diff_score': 0.15,
        'USOSFR10_volatility_score': 0.2,
        'USOSFR10_max_abs_return_score': 0.15
    }
    
    scores_df['composite_score'] = sum(
        scores_df[col] * weight for col, weight in weights.items()
    )
    
    results_df = pd.concat([
        event_metrics_df[['time', 'event', 'ticker']],
        event_metrics_df.drop(columns=['time', 'event', 'ticker']),
        scores_df
    ], axis=1)
    
    return results_df, window_data_df, baseline_df

# Main execution
if __name__ == '__main__':
    # Initialize connection
    today = datetime.now(pytz.timezone('Asia/Hong_Kong'))
    bquery = blp.BlpQuery(
        host='usvxapificcl01.sdi.corp.bankofamerica.com',
        port=8194,
        uuid=32348716,
        ip='165.40.198.238'
    ).start()
    
    # Load data
    sofr_df = load_sofr_data(bquery, '2024-09-26', '2025-04-11')
    events_df = load_economic_events('half_year_data.csv')
    
    # Analyze events
    results_df, window_data_df, baseline_df = analyze_events(sofr_df, events_df)
    
    # Identify significant events (top 10%)
    threshold = results_df['composite_score'].quantile(0.9)
    major_events = results_df[results_df['composite_score'] >= threshold]
    
    # Save and display results
    major_events.to_csv('significant_economic_events_comparison.csv', index=False)
    print(f"Found {len(major_events)} significant economic events:")
    print(major_events[['time', 'event', 'composite_score']].sort_values('composite_score', ascending=False))
    
    # Visualizations
    plot_event_impacts(window_data_df)
    plot_top_events(results_df, window_data_df)
